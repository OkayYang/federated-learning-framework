# -*- coding: utf-8 -*-
# @Author  : xuxiaoyang
# @Time    : 2025/5/16 11:07
# @Describe:
import torch
import numpy as np
from tqdm import tqdm
import torch.nn.functional as F
import torch.nn as nn
from fl.client.fl_base import BaseClient

class FedSPD(BaseClient):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # çŸ¥è¯†è’¸é¦æ ¸å¿ƒå‚æ•°
        self.temperature = kwargs.get('temperature', 1.0)  # æ¸©åº¦å‚æ•°
        self.alpha = kwargs.get('alpha', 0.5)              # logitsè’¸é¦æƒé‡
        self.beta = kwargs.get('beta', 0.3)                # è¡¨å¾è’¸é¦æƒé‡
        self.rep_norm = kwargs.get('rep_norm', True)       # è¡¨å¾å½’ä¸€åŒ–
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')  # KLæ•£åº¦æŸå¤±
        self.mse_loss = nn.MSELoss(reduction='mean')        # MSEæŸå¤±ç”¨äºè¡¨å¾è’¸é¦
        self.cosine_loss = nn.CosineEmbeddingLoss(reduction='mean')  # ä½™å¼¦æŸå¤±

    def _compute_representation_loss(self, student_reps, teacher_reps, loss_type='mse'):
        """
        è®¡ç®—è¡¨å¾è’¸é¦æŸå¤± - é’ˆå¯¹å¼‚æ„æ•°æ®ä¼˜åŒ–
        
        Args:
            student_reps: å­¦ç”Ÿæ¨¡å‹è¡¨å¾
            teacher_reps: æ•™å¸ˆæ¨¡å‹è¡¨å¾
            loss_type: æŸå¤±ç±»å‹ ('mse', 'cosine', 'hybrid')
        """
        # è¡¨å¾å½’ä¸€åŒ– - åœ¨å¼‚æ„æ•°æ®ä¸­å¾ˆé‡è¦
        if self.rep_norm:
            student_reps = F.normalize(student_reps, p=2, dim=1)
            teacher_reps = F.normalize(teacher_reps, p=2, dim=1)
        
        if loss_type == 'mse':
            return self.mse_loss(student_reps, teacher_reps)
        elif loss_type == 'cosine':
            # ä½™å¼¦ç›¸ä¼¼æ€§æŸå¤± - å…³æ³¨æ–¹å‘è€Œéå¹…åº¦
            target = torch.ones(student_reps.size(0), device=student_reps.device)
            return self.cosine_loss(student_reps, teacher_reps, target)
        elif loss_type == 'hybrid':
            # æ··åˆæŸå¤± - ç»“åˆMSEå’Œä½™å¼¦
            mse_loss = self.mse_loss(student_reps, teacher_reps)
            target = torch.ones(student_reps.size(0), device=student_reps.device)
            cosine_loss = self.cosine_loss(student_reps, teacher_reps, target)
            return 0.7 * mse_loss + 0.3 * cosine_loss
        else:
            return self.mse_loss(student_reps, teacher_reps)

    def local_train(self, sync_round: int, weights=None, ensemble_weights=None):
        """
        FedSPDæ•°æ®å¼‚æ„ä¼˜åŒ–ç‰ˆæœ¬
        
        æ ¸å¿ƒè®¾è®¡ï¼š
        - é’ˆå¯¹æ•°æ®å¼‚æ„åœºæ™¯ä¼˜åŒ–è¡¨å¾è’¸é¦
        - åˆ†ç¦»logitså’Œè¡¨å¾è’¸é¦æƒé‡æ§åˆ¶
        - è¡¨å¾å½’ä¸€åŒ–æå‡å¯¹é½æ•ˆæœ
        - æ··åˆæŸå¤±å‡½æ•°æå‡é²æ£’æ€§
        
        æŸå¤±è®¾è®¡ï¼š
        L = CE + Î± * KD_logits + Î² * Rep_loss
        
        å‚æ•°é…ç½®ï¼š
        - Î±: logitsè’¸é¦æƒé‡ (é»˜è®¤0.5)
        - Î²: è¡¨å¾è’¸é¦æƒé‡ (é»˜è®¤0.3)
        - rep_norm: è¡¨å¾å½’ä¸€åŒ– (é»˜è®¤True)
        
        :param weights: æœåŠ¡å™¨ä¼ é€’è¿‡æ¥çš„å½“å‰å…¨å±€æ¨¡å‹æƒé‡
        :param sync_round: å½“å‰çš„é€šä¿¡è½®æ¬¡  
        :param ensemble_weights: æœåŠ¡å™¨ä¼ é€’çš„æ•™å¸ˆæƒé‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        """
        # 1. åŠ è½½å…¨å±€æ¨¡å‹æƒé‡åˆ°æœ¬åœ°æ¨¡å‹
        if weights is not None:
            self.update_weights(weights)
        
        # 2. åˆ›å»ºæ•™å¸ˆæ¨¡å‹ï¼ˆä½¿ç”¨å…¨å±€æƒé‡ï¼‰
        teacher_model = None
        teacher_weights = ensemble_weights if ensemble_weights is not None else weights
        
        if teacher_weights is not None:
            import copy
            from fl.utils import update_model_weights
            
            # åˆ›å»ºæ•™å¸ˆæ¨¡å‹ï¼ˆåŸºäºå…¨å±€æƒé‡ï¼‰
            teacher_model = copy.deepcopy(self.model)
            update_model_weights(teacher_model, teacher_weights)
            teacher_model.eval()
        
        # 3. å¼€å§‹æœ¬åœ°è®­ç»ƒ
        self.model.train()
        total_loss = 0
        num_sample = len(self.train_loader.dataset)
        total_batches = len(self.train_loader) * self.epochs
        
        from fl.utils import ClientProgress
        
        with ClientProgress(
            progress_actor=self.progress_actor,
            client_id=self.client_id,
            total=total_batches,
            desc=f"Client {self.client_id} Training (FedSPD Heterogeneous)"
        ) as pbar:
            for epoch in range(self.epochs):
                epoch_loss = 0
                epoch_ce_loss = 0
                epoch_kd_loss = 0
                epoch_rep_loss = 0
                
                for data, target in self.train_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    self.optimizer.zero_grad()
                    
                    # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                    student_logits = self.model(data)
                    student_reps = None
                    
                    # å¦‚æœæ¨¡å‹æ”¯æŒè¿”å›è¡¨å¾ï¼Œè·å–è¡¨å¾
                    
                    _, student_reps, student_logits = self.model(data, return_all=True)
                        
                    
                    # 1. æœ¬åœ°ç›‘ç£å­¦ä¹ æŸå¤±
                    ce_loss = self.loss(student_logits, target)
                    
                    # 2. ğŸš€ æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼ï¼ˆlogits + è¡¨å¾è’¸é¦ï¼‰
                    kd_loss = torch.tensor(0.0, device=self.device)
                    rep_loss = torch.tensor(0.0, device=self.device)
                    
                    if teacher_model is not None:
                        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
                        with torch.no_grad():
                            teacher_logits = teacher_model(data)
                            teacher_reps = None
                            
                            # è·å–æ•™å¸ˆè¡¨å¾
                            
                            _, teacher_reps, teacher_logits = teacher_model(data, return_all=True)
                            
                        
                        # Logitsè’¸é¦
                        with torch.no_grad():
                            teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)
                        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)
                        kd_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.temperature ** 2)
                        
                        # ğŸ¯ å¼‚æ„æ•°æ®ä¼˜åŒ–çš„è¡¨å¾è’¸é¦
                        if student_reps is not None and teacher_reps is not None:
                            rep_loss = self._compute_representation_loss(
                                student_reps, teacher_reps, loss_type='hybrid'
                            )
                    
                    # 3. å¼‚æ„æ•°æ®ä¼˜åŒ–æ€»æŸå¤±ï¼šL = CE + Î± * KD + Î² * Rep
                    total_batch_loss = ce_loss + self.alpha * kd_loss + self.beta * rep_loss
                    
                    # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                    total_batch_loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª - åœ¨å¼‚æ„æ•°æ®ä¸­å¾ˆé‡è¦
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    
                    self.optimizer.step()
                    
                    # è®°å½•æŸå¤±
                    epoch_loss += total_batch_loss.item()
                    epoch_ce_loss += ce_loss.item()
                    epoch_kd_loss += kd_loss.item()
                    epoch_rep_loss += rep_loss.item()

                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(1)
                    
                total_loss += epoch_loss
                avg_loss = epoch_loss / len(self.train_loader)
                avg_ce_loss = epoch_ce_loss / len(self.train_loader)
                avg_kd_loss = epoch_kd_loss / len(self.train_loader)
                avg_rep_loss = epoch_rep_loss / len(self.train_loader)
                current_lr = self.optimizer.param_groups[0]['lr']

                # æ‰“å°æŸå¤±ä¿¡æ¯
                pbar.set_postfix({
                    'epoch': f"{epoch+1}/{self.epochs}",
                    'total': f"{avg_loss:.4f}",
                    'ce': f"{avg_ce_loss:.4f}",
                    'kd': f"{avg_kd_loss:.4f}",
                    'rep': f"{avg_rep_loss:.4f}",
                    'Î±': f"{self.alpha:.1f}",
                    'Î²': f"{self.beta:.1f}",
                    'lr': f"{current_lr:.6f}"
                })
        
        self.scheduler.step()
        
        # è·å–è®­ç»ƒåçš„æƒé‡
        model_weights = self.get_weights(return_numpy=True)

        # è¿”å›æ›´æ–°åçš„æƒé‡ã€æ ·æœ¬æ•°ã€å¹³å‡æŸå¤±
        avg_loss = total_loss / (len(self.train_loader) * self.epochs)
        return model_weights, num_sample, avg_loss